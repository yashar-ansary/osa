---
cidr_networks:
  container: 172.16.16.0/24
  tunnel: 172.16.17.0/24
  storage: 172.16.18.0/24

used_ips:
  - "172.16.16.1,172.16.16.114"
  - "172.16.17.100,172.16.17.116"
  - "172.16.18.100,172.16.18.116"

global_overrides:
  internal_lb_vip_address: 172.16.16.9
  #
  # The below domain name must resolve to an IP address
  # in the CIDR specified in haproxy_keepalived_external_vip_cidr.
  # If using different protocols (https/http) for the public/internal
  # endpoints the two addresses must be different.
  #
  external_lb_vip_address: 172.16.16.10
  management_bridge: "br-mgmt"
  provider_networks:
    - network:
        container_bridge: "br-mgmt"
        container_type: "veth"
        container_interface: "mgmt"
        ip_from_q: "container"
        type: "raw"
        group_binds:
          - all_containers
          - hosts
        is_container_address: true
    - network:
        container_bridge: "br-vxlan"
        container_type: "veth"
        container_interface: "vxlan"
        ip_from_q: "tunnel"
        type: "vxlan"
        range: "1:1000"
        net_name: "vxlan"
        group_binds:
          - neutron_openvswitch_agent
    - network:
        container_bridge: "br-provider"
        container_type: "veth"
        container_interface: "eth12"
        host_bind_override: "eth0"
        type: "flat"
        net_name: "flat"
        group_binds:
          - neutron_openvswitch_agent
    - network:
        container_bridge: "br-storage"
        container_type: "veth"
        container_interface: "storage"
        ip_from_q: "storage"
        type: "raw"
        group_binds:
          - glance_api
          - cinder_api
          - cinder_volume
          - nova_compute

###
### Infrastructure
###

# galera, memcache, rabbitmq, utility
shared-infra_hosts:
  controller1:
    ip: 172.16.16.111
  controller2:
    ip: 172.16.16.112
  controller3:
    ip: 172.16.16.113

# repository (apt cache, python packages, etc)
repo-infra_hosts:
  controller1:
    ip: 172.16.16.111
  controller2:
    ip: 172.16.16.112
  controller3:
    ip: 172.16.16.113

# load balancer
# Ideally the load balancer should not use the Infrastructure hosts.
# Dedicated hardware is best for improved performance and security.
haproxy_hosts:
  controller1:
    ip: 172.16.16.111
  controller2:
    ip: 172.16.16.112
  controller3:
    ip: 172.16.16.113

# rsyslog server
log_hosts:
  log1:
    ip: 172.16.16.12

###
### OpenStack
###

# keystone
identity_hosts:
  controller1:
    ip: 172.16.16.111
  controller2:
    ip: 172.16.16.112
  controller3:
    ip: 172.16.16.113

# cinder api services
storage-infra_hosts:
  controller1:
    ip: 172.16.16.111
  controller2:
    ip: 172.16.16.112
  controller3:
    ip: 172.16.16.113

# glance
# The settings here are repeated for each infra host.
# They could instead be applied as global settings in
# user_variables, but are left here to illustrate that
# each container could have different storage targets.

image_hosts:
  controller1:
    ip: 172.16.16.111
    # container_vars:
    #   limit_container_types: glance
    #   glance_nfs_client:
    #     - server: "172.29.244.15"
    #       remote_path: "/images"
    #       local_path: "/var/lib/glance/images"
    #       type: "nfs"
    #       options: "_netdev,auto"
  controller2:
    ip: 172.16.16.112
    # container_vars:
    #   limit_container_types: glance
    #   glance_nfs_client:
    #     - server: "172.29.244.15"
    #       remote_path: "/images"
    #       local_path: "/var/lib/glance/images"
    #       type: "nfs"
    #       options: "_netdev,auto"
  controller3:
    ip: 172.16.16.113
    # container_vars:
    #   limit_container_types: glance
    #   glance_nfs_client:
    #     - server: "172.29.244.15"
    #       remote_path: "/images"
    #       local_path: "/var/lib/glance/images"
    #       type: "nfs"
    #       options: "_netdev,auto"

# placement
placement-infra_hosts:
  controller1:
    ip: 172.16.16.111
  controller2:
    ip: 172.16.16.112
  controller3:
    ip: 172.16.16.113

# nova api, conductor, etc services
compute-infra_hosts:
  controller1:
    ip: 172.16.16.111
  controller2:
    ip: 172.16.16.112
  controller3:
    ip: 172.16.16.113

# heat
orchestration_hosts:
  controller1:
    ip: 172.16.16.111
  controller2:
    ip: 172.16.16.112
  controller3:
    ip: 172.16.16.113

# horizon
dashboard_hosts:
  controller1:
    ip: 172.16.16.111
  controller2:
    ip: 172.16.16.112
  controller3:
    ip: 172.16.16.113

# neutron api
# network-infra_hosts:
#   controller1:
#     ip: 172.16.16.111
#   controller2:
#     ip: 172.16.16.112
#   controller3:
#     ip: 172.16.16.113

network_hosts:
  controller1:
    ip: 172.16.16.111
  controller2:
    ip: 172.16.16.112
  controller3:
    ip: 172.16.16.113

# neutron agents (L3, DHCP, etc)
# network-agent_hosts:
#   controller1:
#     ip: 172.16.16.111
#   controller2:
#     ip: 172.16.16.112
#   controller3:
#     ip: 172.16.16.113

# ceilometer (telemetry data collection)
metering-infra_hosts:
  controller1:
    ip: 172.16.16.111
  controller2:
    ip: 172.16.16.112
  controller3:
    ip: 172.16.16.113

# aodh (telemetry alarm service)
metering-alarm_hosts:
  controller1:
    ip: 172.16.16.111
  controller2:
    ip: 172.16.16.112
  controller3:
    ip: 172.16.16.113

# gnocchi (telemetry metrics storage)
metrics_hosts:
  controller1:
    ip: 172.16.16.111
  controller2:
    ip: 172.16.16.112
  controller3:
    ip: 172.16.16.113

# nova hypervisors
compute_hosts:
  compute-01:
    ip: 172.16.16.114


# ceilometer compute agent (telemetry data collection)
metering-compute_hosts:
  compute-01:
    ip: 172.16.16.114


# cinder volume hosts (NFS-backed)
# The settings here are repeated for each infra host.
# They could instead be applied as global settings in
# user_variables, but are left here to illustrate that
# each container could have different storage targets.

# storage_hosts:
#   controller1:
#     ip: 172.16.16.111
#     container_vars:
#       cinder_backends:
#         limit_container_types: cinder_volume
#         nfs_volume:
#           volume_backend_name: NFS_VOLUME1
#           volume_driver: cinder.volume.drivers.nfs.NfsDriver
#           nfs_mount_options: "rsize=65535,wsize=65535,timeo=1200,actimeo=120"
#           nfs_shares_config: /etc/cinder/nfs_shares
#           shares:
#             - ip: "172.29.244.15"
#               share: "/vol/cinder"
#   controller2:
#     ip: 172.16.16.112
#     container_vars:
#       cinder_backends:
#         limit_container_types: cinder_volume
#         nfs_volume:
#           volume_backend_name: NFS_VOLUME1
#           volume_driver: cinder.volume.drivers.nfs.NfsDriver
#           nfs_mount_options: "rsize=65535,wsize=65535,timeo=1200,actimeo=120"
#           nfs_shares_config: /etc/cinder/nfs_shares
#           shares:
#             - ip: "172.29.244.15"
#               share: "/vol/cinder"
#   controller3:
#     ip: 172.16.16.113
#     container_vars:
#       cinder_backends:
#         limit_container_types: cinder_volume
#         nfs_volume:
#           volume_backend_name: NFS_VOLUME1
#           volume_driver: cinder.volume.drivers.nfs.NfsDriver
#           nfs_mount_options: "rsize=65535,wsize=65535,timeo=1200,actimeo=120"
#           nfs_shares_config: /etc/cinder/nfs_shares
#           shares:
#             - ip: "172.29.244.15"
#               share: "/vol/cinder"
